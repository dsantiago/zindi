{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch \n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.svm import *\n",
    "# from sklearn.linear_model import *\n",
    "# from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 3), (30000, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treino, teste e modelo de submissão\n",
    "train = pd.read_csv('data/Train.csv')\n",
    "test = pd.read_csv('data/Test.csv')\n",
    "sub = pd.read_csv('data/SampleSubmission.csv')\n",
    "\n",
    "# Parece que os IDs do modelo de submissão estão fora de ordem\n",
    "# então substituímos eles pela ordem do arquivo de teste\n",
    "sub.ID = test.ID\n",
    "\n",
    "# Target a ser previsto o treino\n",
    "target = train['label']\n",
    "\n",
    "# Dimensões dos dataframes de treino e teste\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessamento básico\n",
    "def preprocess(txt):   \n",
    "    txt = re.sub(r' +', ' ', txt)\n",
    "    txt = txt.lower()\n",
    "    return txt\n",
    "\n",
    "def remove_sw(corpus, stop_words):\n",
    "    corpus = corpus.apply(lambda x: [e for e in x.split(' ') if e not in stop_words])\n",
    "    corpus = corpus.str.join(' ')\n",
    "    return corpus\n",
    "\n",
    "train['txt_ok'] = train.text.apply(preprocess)\n",
    "test['txt_ok'] = test.text.apply(preprocess)\n",
    "\n",
    "REMOVE_STOPWORDS = True\n",
    "# REMOVE_STOPWORDS = False\n",
    "\n",
    "if REMOVE_STOPWORDS:\n",
    "    THRESHOLD = 5000\n",
    "    words = train.text.str.split(' ').explode()\n",
    "    stop_words = words.value_counts()[words.value_counts() > THRESHOLD].index.tolist()\n",
    "    train['txt_ok'] = remove_sw(train['txt_ok'], stop_words=stop_words)\n",
    "    test['txt_ok'] = remove_sw(test['txt_ok'], stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo o conjunto de textos\n",
    "corpus = pd.concat([train.txt_ok, test.txt_ok], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorização: transformação dos textos em uma matriz numérica\n",
    "# de documentos x termos.\n",
    "\n",
    "N_FEATURES = 1000000 // 10\n",
    "\n",
    "vec = TfidfVectorizer(max_features=N_FEATURES, ngram_range=(1, 3), sublinear_tf=False)\n",
    "vec.fit(corpus)\n",
    "train_vec = vec.transform(train['txt_ok'])\n",
    "test_vec = vec.transform(test['txt_ok'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 100000), (70000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(Dataset):\n",
    "    def __init__(self, data, label, n_features):\n",
    "        super(DS, self).__init__()\n",
    "        self.data = data   #torch.from_numpy(data)\n",
    "        self.label = label #torch.from_numpy(label)\n",
    "        self.data_len = data.shape[0]\n",
    "        self.n_features = n_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_row = self.data[idx, :].nonzero()[1]\n",
    "        X = np.zeros(self.n_features)\n",
    "        X[idx_row] = self.data[idx, idx_row].toarray().flatten()\n",
    "\n",
    "        X = torch.FloatTensor(X)\n",
    "        y = self.label[idx]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net2(torch.nn.Module):\n",
    "#     def __init__(self, n_features):\n",
    "#         super().__init__()\n",
    "#         self.hidden = 600\n",
    "#         self.fc1 = torch.nn.Linear(n_features, self.hidden)\n",
    "#         self.fc2 = torch.nn.Linear(self.hidden, self.hidden // 2)\n",
    "#         self.fc3 = torch.nn.Linear(self.hidden // 2, 3)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, p=0.4, training=self.training)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.dropout(x, p=0.4, training=self.training)\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, n_features, n_classes=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = 600\n",
    "        self.fc1 = torch.nn.Linear(n_features, self.hidden)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden, self.hidden // 2)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden // 2, n_classes)\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.4, training=training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.4, training=training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x, training=True)\n",
    "        crit = torch.nn.CrossEntropyLoss()\n",
    "#         loss = F.cross_entropy(y_hat, y)\n",
    "        loss = crit(y_hat, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.cross_entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DS(train_vec, target + 1, n_features=N_FEATURES)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X, y in train_loader:\n",
    "#     break\n",
    "    \n",
    "# X.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 60 M  \n",
      "1 | fc2  | Linear | 180 K \n",
      "2 | fc3  | Linear | 903   \n",
      "/Users/santiago/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492b3559c6fb4d9bbf2ec780fb14438c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = DS(train_vec, target + 1, n_features=N_FEATURES)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "model = Net(n_features=N_FEATURES)\n",
    "# trainer = pl.Trainer(max_steps=3)\n",
    "trainer = pl.Trainer(max_steps=20, progress_bar_refresh_rate=10)\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TextSentiment(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim, num_class):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "#         self.fc = nn.Linear(embed_dim, n\n",
    "#                             um_class)\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         initrange = 0.5\n",
    "#         self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.fc.bias.data.zero_()\n",
    "\n",
    "#     def forward(self, text, offsets):\n",
    "#         embedded = self.embedding(text, offsets)\n",
    "#         return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
